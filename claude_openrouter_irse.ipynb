{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad144be8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# requirements.txt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModel, AutoModelForSequenceClassification,\n",
        "    TrainingArguments, Trainer, BertTokenizer, BertModel\n",
        ")\n",
        "import requests  # For OpenRouter API calls\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "import re\n",
        "import ast\n",
        "import warnings\n",
        "import os\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Set your OpenRouter API key here\n",
        "OPENROUTER_API_KEY = \"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\"  # Replace with your actual API key\n",
        "# Or set it as environment variable: OPENROUTER_API_KEY = os.getenv('OPENROUTER_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12a08c1c",
      "metadata": {},
      "outputs": [],
      "source": [
        "class DatasetAnalyzer:\n",
        "    def __init__(self, seed_data_path, humaneval_path):\n",
        "        self.seed_data_path = seed_data_path\n",
        "        self.humaneval_path = humaneval_path\n",
        "        self.seed_df = None\n",
        "        self.humaneval_data = None\n",
        "    \n",
        "    def load_seed_dataset(self):\n",
        "        \"\"\"Load the seed dataset with comments\"\"\"\n",
        "        self.seed_df = pd.read_csv(self.seed_data_path)\n",
        "        print(f\"Loaded seed dataset: {len(self.seed_df)} samples\")\n",
        "        return self.seed_df\n",
        "    \n",
        "    def load_humaneval_dataset(self):\n",
        "        \"\"\"Load HumanEval dataset\"\"\"\n",
        "        with open(self.humaneval_path, 'r') as f:\n",
        "            self.humaneval_data = [json.loads(line) for line in f]\n",
        "        print(f\"Loaded HumanEval dataset: {len(self.humaneval_data)} tasks\")\n",
        "        return self.humaneval_data\n",
        "    \n",
        "    def analyze_seed_dataset(self):\n",
        "        \"\"\"Comprehensive analysis of seed dataset\"\"\"\n",
        "        print(\"=== SEED DATASET ANALYSIS ===\")\n",
        "        \n",
        "        # Basic statistics\n",
        "        print(f\"Total samples: {len(self.seed_df)}\")\n",
        "        print(f\"Label distribution:\")\n",
        "        print(self.seed_df['label'].value_counts())\n",
        "        print(f\"Label distribution (%):\")\n",
        "        print(self.seed_df['label'].value_counts(normalize=True) * 100)\n",
        "        \n",
        "        # Comment length analysis\n",
        "        self.seed_df['comment_length'] = self.seed_df['comment_text'].str.len()\n",
        "        self.seed_df['code_length'] = self.seed_df['code_snippet'].str.len()\n",
        "        \n",
        "        # Visualization\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "        \n",
        "        # Label distribution\n",
        "        self.seed_df['label'].value_counts().plot(kind='bar', ax=axes[0,0])\n",
        "        axes[0,0].set_title('Label Distribution')\n",
        "        axes[0,0].set_xlabel('Label')\n",
        "        axes[0,0].set_ylabel('Count')\n",
        "        \n",
        "        # Comment length distribution by label\n",
        "        sns.boxplot(data=self.seed_df, x='label', y='comment_length', ax=axes[0,1])\n",
        "        axes[0,1].set_title('Comment Length by Label')\n",
        "        \n",
        "        # Code length distribution by label\n",
        "        sns.boxplot(data=self.seed_df, x='label', y='code_length', ax=axes[1,0])\n",
        "        axes[1,0].set_title('Code Length by Label')\n",
        "        \n",
        "        # Comment length histogram\n",
        "        self.seed_df['comment_length'].hist(bins=50, ax=axes[1,1])\n",
        "        axes[1,1].set_title('Comment Length Distribution')\n",
        "        axes[1,1].set_xlabel('Comment Length')\n",
        "        axes[1,1].set_ylabel('Frequency')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.savefig('seed_dataset_analysis.png', dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "        \n",
        "        # Statistical summary\n",
        "        print(\"\\nComment Length Statistics:\")\n",
        "        print(self.seed_df.groupby('label')['comment_length'].describe())\n",
        "        \n",
        "        print(\"\\nCode Length Statistics:\")\n",
        "        print(self.seed_df.groupby('label')['code_length'].describe())\n",
        "        \n",
        "        return self.seed_df\n",
        "    \n",
        "    def analyze_humaneval_dataset(self):\n",
        "        \"\"\"Analyze HumanEval dataset structure\"\"\"\n",
        "        print(\"\\n=== HUMANEVAL DATASET ANALYSIS ===\")\n",
        "        \n",
        "        sample_task = self.humaneval_data[0]\n",
        "        print(\"Sample task structure:\")\n",
        "        for key, value in sample_task.items():\n",
        "            if isinstance(value, str) and len(value) > 100:\n",
        "                print(f\"{key}: {value[:100]}...\")\n",
        "            else:\n",
        "                print(f\"{key}: {value}\")\n",
        "        \n",
        "        return self.humaneval_data\n",
        "\n",
        "# Initialize analyzer\n",
        "analyzer = DatasetAnalyzer('Code_Comment_Seed_Data.csv', 'humaneval_dataset.jsonl')\n",
        "seed_df = analyzer.load_seed_dataset()\n",
        "# humaneval_data = analyzer.load_humaneval_dataset()  # Uncomment if you have the file\n",
        "seed_analysis = analyzer.analyze_seed_dataset()\n",
        "# humaneval_analysis = analyzer.analyze_humaneval_dataset()  # Uncomment if you have the file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea6187ec",
      "metadata": {},
      "outputs": [],
      "source": [
        "class OpenRouterLLMLabeler:\n",
        "    def __init__(self, api_key, model=\"google/gemini-2.5-pro-exp-03-25\"):\n",
        "        self.api_key = api_key\n",
        "        self.model = model\n",
        "        self.base_url = \"https://openrouter.ai/api/v1/chat/completions\"\n",
        "        self.headers = {\n",
        "            \"Authorization\": f\"Bearer {api_key}\",\n",
        "            #\"HTTP-Referer\": \"https://github.com/your-repo\",  # Optional\n",
        "            \"X-Title\": \"IRSE Comment Usefulness Classification\",  # Optional\n",
        "            \"Content-Type\": \"application/json\"\n",
        "        }\n",
        "        \n",
        "        self.labeling_prompt = \"\"\"\n",
        "        You are an expert software developer tasked with evaluating code comments.\n",
        "        \n",
        "        A useful comment should:\n",
        "        - Explain WHY something is done, not just WHAT is done\n",
        "        - Provide context that isn't obvious from the code\n",
        "        - Help future maintainers understand the purpose\n",
        "        - Explain complex algorithms or business logic\n",
        "        - Warn about edge cases or important considerations\n",
        "        \n",
        "        A comment is NOT useful if it:\n",
        "        - Simply restates what the code does\n",
        "        - States the obvious\n",
        "        - Is outdated or incorrect\n",
        "        - Provides no additional value\n",
        "        \n",
        "        Given the following code snippet and comment, classify the comment as \"useful\" or \"not useful\".\n",
        "        \n",
        "        Code:\n",
        "        {code}\n",
        "        \n",
        "        Comment:\n",
        "        {comment}\n",
        "        \n",
        "        Classification (respond with only \"useful\" or \"not useful\"):\n",
        "        \"\"\"\n",
        "    \n",
        "    def make_api_call(self, messages, temperature=0.1):\n",
        "        \"\"\"Make API call to OpenRouter\"\"\"\n",
        "        data = {\n",
        "            \"model\": self.model,\n",
        "            \"messages\": messages,\n",
        "            \"temperature\": temperature\n",
        "        }\n",
        "        \n",
        "        try:\n",
        "            response = requests.post(self.base_url, headers=self.headers, json=data)\n",
        "            response.raise_for_status()\n",
        "            return response.json()\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"API call failed: {e}\")\n",
        "            return None\n",
        "    \n",
        "    def generate_synthetic_comments(self, code_snippet, num_comments=3):\n",
        "        \"\"\"Generate synthetic comments for a code snippet\"\"\"\n",
        "        prompt = f\"\"\"\n",
        "        Generate {num_comments} different types of comments for this code snippet:\n",
        "        1. One useful comment explaining the purpose/why\n",
        "        2. One not useful comment that just restates what the code does\n",
        "        3. One moderately useful comment\n",
        "        \n",
        "        Code:\n",
        "        {code_snippet}\n",
        "        \n",
        "        Format your response as JSON:\n",
        "        {{\"comments\": [{{\"text\": \"comment1\", \"type\": \"useful\"}}, {{\"text\": \"comment2\", \"type\": \"not_useful\"}}, {{\"text\": \"comment3\", \"type\": \"moderate\"}}]}}\n",
        "        \"\"\"\n",
        "        \n",
        "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "        response = self.make_api_call(messages, temperature=0.7)\n",
        "        \n",
        "        if response and 'choices' in response:\n",
        "            try:\n",
        "                content = response['choices'][0]['message']['content']\n",
        "                return json.loads(content)\n",
        "            except (json.JSONDecodeError, KeyError) as e:\n",
        "                print(f\"Error parsing response: {e}\")\n",
        "                return None\n",
        "        return None\n",
        "    \n",
        "    def label_comment(self, code, comment):\n",
        "        \"\"\"Label a single comment as useful or not useful\"\"\"\n",
        "        prompt = self.labeling_prompt.format(code=code, comment=comment)\n",
        "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "        \n",
        "        response = self.make_api_call(messages, temperature=0.1)\n",
        "        \n",
        "        if response and 'choices' in response:\n",
        "            try:\n",
        "                label = response['choices'][0]['message']['content'].strip().lower()\n",
        "                return \"useful\" if \"useful\" in label and \"not\" not in label else \"not_useful\"\n",
        "            except KeyError as e:\n",
        "                print(f\"Error extracting label: {e}\")\n",
        "                return None\n",
        "        return None\n",
        "    \n",
        "    def process_seed_dataset_sample(self, seed_df, sample_size=100):\n",
        "        \"\"\"Process a sample of the seed dataset to validate labeling\"\"\"\n",
        "        print(f\"Processing {sample_size} samples from seed dataset for validation...\")\n",
        "        \n",
        "        # Sample data\n",
        "        sample_df = seed_df.sample(n=min(sample_size, len(seed_df)), random_state=42)\n",
        "        predictions = []\n",
        "        \n",
        "        for idx, row in tqdm(sample_df.iterrows(), total=len(sample_df), desc=\"Labeling comments\"):\n",
        "            prediction = self.label_comment(row['code_snippet'], row['comment_text'])\n",
        "            predictions.append(prediction)\n",
        "        \n",
        "        sample_df = sample_df.copy()\n",
        "        sample_df['llm_prediction'] = predictions\n",
        "        \n",
        "        # Calculate agreement\n",
        "        # Map labels for comparison\n",
        "        label_map = {'Useful': 'useful', 'Not Useful': 'not_useful'}\n",
        "        sample_df['true_label_mapped'] = sample_df['label'].map(label_map)\n",
        "        \n",
        "        valid_predictions = sample_df.dropna(subset=['llm_prediction'])\n",
        "        if len(valid_predictions) > 0:\n",
        "            agreement = (valid_predictions['true_label_mapped'] == valid_predictions['llm_prediction']).mean()\n",
        "            print(f\"LLM-Human agreement: {agreement:.3f}\")\n",
        "        \n",
        "        return sample_df\n",
        "\n",
        "# Initialize OpenRouter labeler\n",
        "if OPENROUTER_API_KEY and OPENROUTER_API_KEY != \"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\":\n",
        "    labeler = OpenRouterLLMLabeler(api_key=OPENROUTER_API_KEY, model=\"google/gemini-2.0-flash-exp:free\")  # Using cheaper model\n",
        "    \n",
        "    # Test with a small sample\n",
        "    validation_sample = labeler.process_seed_dataset_sample(seed_df, sample_size=10)\n",
        "    print(\"\\nValidation sample results:\")\n",
        "    print(validation_sample[['comment_text', 'label', 'llm_prediction']].head())\n",
        "else:\n",
        "    print(\"Please set your OpenRouter API key to use LLM labeling functionality\")\n",
        "    labeler = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "806efd2a",
      "metadata": {},
      "outputs": [],
      "source": [
        "class DataPreprocessor:\n",
        "    def __init__(self):\n",
        "        self.label_encoder = {'Useful': 1, 'Not Useful': 0, 'useful': 1, 'not_useful': 0}\n",
        "        self.feature_extractors = {}\n",
        "    \n",
        "    def extract_features(self, df):\n",
        "        \"\"\"Extract handcrafted features from code and comments\"\"\"\n",
        "        features = pd.DataFrame()\n",
        "        \n",
        "        # Comment features\n",
        "        features['comment_length'] = df['comment_text'].str.len()\n",
        "        features['comment_words'] = df['comment_text'].str.split().str.len()\n",
        "        features['has_question_mark'] = df['comment_text'].str.contains('\\?').astype(int)\n",
        "        features['has_exclamation'] = df['comment_text'].str.contains('!').astype(int)\n",
        "        features['starts_with_verb'] = df['comment_text'].str.lower().str.startswith(\n",
        "            ('return', 'get', 'set', 'create', 'update', 'delete', 'check', 'validate')\n",
        "        ).astype(int)\n",
        "        \n",
        "        # Code features\n",
        "        features['code_length'] = df['code_snippet'].str.len()\n",
        "        features['code_lines'] = df['code_snippet'].str.count('\\n') + 1\n",
        "        features['has_loops'] = df['code_snippet'].str.contains('for |while ').astype(int)\n",
        "        features['has_conditionals'] = df['code_snippet'].str.contains('if |elif |else:').astype(int)\n",
        "        features['has_functions'] = df['code_snippet'].str.contains('def ').astype(int)\n",
        "        features['has_classes'] = df['code_snippet'].str.contains('class ').astype(int)\n",
        "        \n",
        "        # Complexity features\n",
        "        features['cyclomatic_complexity'] = self.calculate_complexity(df['code_snippet'])\n",
        "        features['comment_code_ratio'] = features['comment_length'] / (features['code_length'] + 1)\n",
        "        \n",
        "        # Semantic features\n",
        "        features['has_explanatory_words'] = df['comment_text'].str.lower().str.contains(\n",
        "            'why|because|purpose|reason|algorithm|optimization'\n",
        "        ).astype(int)\n",
        "        \n",
        "        features['has_obvious_words'] = df['comment_text'].str.lower().str.contains(\n",
        "            'this function|this method|this code|returns|sets|gets'\n",
        "        ).astype(int)\n",
        "        \n",
        "        return features\n",
        "    \n",
        "    def calculate_complexity(self, code_series):\n",
        "        \"\"\"Calculate cyclomatic complexity approximation\"\"\"\n",
        "        complexity = []\n",
        "        for code in code_series:\n",
        "            if pd.isna(code):\n",
        "                complexity.append(1)\n",
        "                continue\n",
        "            # Count decision points\n",
        "            decisions = (\n",
        "                code.count('if ') + code.count('elif ') + \n",
        "                code.count('for ') + code.count('while ') +\n",
        "                code.count('except ') + code.count('and ') + code.count('or ')\n",
        "            )\n",
        "            complexity.append(decisions + 1)  # Base complexity is 1\n",
        "        return complexity\n",
        "    \n",
        "    def prepare_datasets(self, df):\n",
        "        \"\"\"Prepare datasets for training\"\"\"\n",
        "        df = df.copy()\n",
        "        \n",
        "        # Clean data\n",
        "        df = df.dropna(subset=['comment_text', 'code_snippet', 'label'])\n",
        "        \n",
        "        # Extract features\n",
        "        features = self.extract_features(df)\n",
        "        \n",
        "        # Add features to dataframe\n",
        "        for col in features.columns:\n",
        "            df[col] = features[col]\n",
        "        \n",
        "        # Encode labels\n",
        "        df['label_encoded'] = df['label'].map(self.label_encoder)\n",
        "        \n",
        "        # Split data\n",
        "        train_df, temp_df = train_test_split(\n",
        "            df, test_size=0.3, random_state=42, \n",
        "            stratify=df['label_encoded']\n",
        "        )\n",
        "        val_df, test_df = train_test_split(\n",
        "            temp_df, test_size=0.5, random_state=42,\n",
        "            stratify=temp_df['label_encoded']\n",
        "        )\n",
        "        \n",
        "        print(f\"Training set: {len(train_df)} samples\")\n",
        "        print(f\"Validation set: {len(val_df)} samples\") \n",
        "        print(f\"Test set: {len(test_df)} samples\")\n",
        "        \n",
        "        return {\n",
        "            'train': train_df,\n",
        "            'val': val_df,\n",
        "            'test': test_df,\n",
        "            'features': features.columns.tolist(),\n",
        "            'combined': df\n",
        "        }\n",
        "\n",
        "# Prepare datasets\n",
        "preprocessor = DataPreprocessor()\n",
        "datasets = preprocessor.prepare_datasets(seed_df)\n",
        "print(f\"\\nFeature columns: {datasets['features']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d19143fd",
      "metadata": {},
      "outputs": [],
      "source": [
        "class BaselineModels:\n",
        "    def __init__(self):\n",
        "        self.models = {}\n",
        "        self.vectorizers = {}\n",
        "        self.results = {}\n",
        "    \n",
        "    def rule_based_baseline(self, df):\n",
        "        \"\"\"Simple rule-based baseline\"\"\"\n",
        "        predictions = []\n",
        "        for _, row in df.iterrows():\n",
        "            comment = str(row['comment_text']).lower()\n",
        "            code = str(row['code_snippet']).lower()\n",
        "            \n",
        "            # Simple heuristics\n",
        "            score = 0\n",
        "            \n",
        "            # Useful indicators\n",
        "            if any(word in comment for word in ['why', 'because', 'purpose', 'reason']):\n",
        "                score += 2\n",
        "            if any(word in comment for word in ['algorithm', 'optimization', 'performance']):\n",
        "                score += 2\n",
        "            if any(word in comment for word in ['todo', 'fixme', 'hack', 'temporary']):\n",
        "                score += 1\n",
        "            if len(comment.split()) > 10:  # Longer comments tend to be more useful\n",
        "                score += 1\n",
        "            \n",
        "            # Not useful indicators\n",
        "            if comment.startswith(('return', 'get', 'set', 'create')):\n",
        "                score -= 2\n",
        "            if any(phrase in comment for phrase in ['this function', 'this method', 'this code']):\n",
        "                score -= 1\n",
        "            if len(comment.split()) < 3:  # Very short comments\n",
        "                score -= 1\n",
        "            \n",
        "            predictions.append(1 if score > 0 else 0)\n",
        "        \n",
        "        return np.array(predictions)\n",
        "    \n",
        "    def tfidf_svm_baseline(self, train_df, val_df, test_df):\n",
        "        \"\"\"TF-IDF + SVM baseline\"\"\"\n",
        "        # Combine comment and code text\n",
        "        def combine_text(row):\n",
        "            return f\"{row['comment_text']} [SEP] {row['code_snippet']}\"\n",
        "        \n",
        "        train_text = train_df.apply(combine_text, axis=1)\n",
        "        val_text = val_df.apply(combine_text, axis=1)\n",
        "        test_text = test_df.apply(combine_text, axis=1)\n",
        "        \n",
        "        # TF-IDF vectorization\n",
        "        vectorizer = TfidfVectorizer(max_features=10000, stop_words='english', ngram_range=(1, 2))\n",
        "        X_train = vectorizer.fit_transform(train_text)\n",
        "        X_val = vectorizer.transform(val_text)\n",
        "        X_test = vectorizer.transform(test_text)\n",
        "        \n",
        "        # Train SVM\n",
        "        svm_model = SVC(kernel='rbf', random_state=42, class_weight='balanced')\n",
        "        svm_model.fit(X_train, train_df['label_encoded'])\n",
        "        \n",
        "        # Predictions\n",
        "        val_pred = svm_model.predict(X_val)\n",
        "        test_pred = svm_model.predict(X_test)\n",
        "        \n",
        "        self.models['tfidf_svm'] = svm_model\n",
        "        self.vectorizers['tfidf'] = vectorizer\n",
        "        \n",
        "        return val_pred, test_pred\n",
        "    \n",
        "    def random_forest_baseline(self, train_df, val_df, test_df, feature_cols):\n",
        "        \"\"\"Random Forest with handcrafted features\"\"\"\n",
        "        X_train = train_df[feature_cols].fillna(0)\n",
        "        X_val = val_df[feature_cols].fillna(0)\n",
        "        X_test = test_df[feature_cols].fillna(0)\n",
        "        \n",
        "        # Train Random Forest\n",
        "        rf_model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
        "        rf_model.fit(X_train, train_df['label_encoded'])\n",
        "        \n",
        "        # Predictions\n",
        "        val_pred = rf_model.predict(X_val)\n",
        "        test_pred = rf_model.predict(X_test)\n",
        "        \n",
        "        self.models['random_forest'] = rf_model\n",
        "        \n",
        "        # Feature importance\n",
        "        feature_importance = pd.DataFrame({\n",
        "            'feature': feature_cols,\n",
        "            'importance': rf_model.feature_importances_\n",
        "        }).sort_values('importance', ascending=False)\n",
        "        \n",
        "        print(\"Top 10 most important features:\")\n",
        "        print(feature_importance.head(10))\n",
        "        \n",
        "        return val_pred, test_pred\n",
        "    \n",
        "    def evaluate_model(self, y_true, y_pred, model_name):\n",
        "        \"\"\"Evaluate model performance\"\"\"\n",
        "        accuracy = accuracy_score(y_true, y_pred)\n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
        "        \n",
        "        results = {\n",
        "            'accuracy': accuracy,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1': f1\n",
        "        }\n",
        "        \n",
        "        self.results[model_name] = results\n",
        "        \n",
        "        print(f\"\\n{model_name} Results:\")\n",
        "        print(f\"Accuracy: {accuracy:.4f}\")\n",
        "        print(f\"Precision: {precision:.4f}\")\n",
        "        print(f\"Recall: {recall:.4f}\")\n",
        "        print(f\"F1-Score: {f1:.4f}\")\n",
        "        \n",
        "        return results\n",
        "\n",
        "# Train baseline models\n",
        "baselines = BaselineModels()\n",
        "\n",
        "print(\"Training baseline models...\\n\")\n",
        "\n",
        "# Rule-based baseline\n",
        "print(\"1. Rule-based baseline:\")\n",
        "rule_pred_val = baselines.rule_based_baseline(datasets['val'])\n",
        "rule_pred_test = baselines.rule_based_baseline(datasets['test'])\n",
        "baselines.evaluate_model(datasets['val']['label_encoded'], rule_pred_val, 'Rule-based')\n",
        "\n",
        "# TF-IDF + SVM baseline\n",
        "print(\"\\n2. TF-IDF + SVM baseline:\")\n",
        "svm_val_pred, svm_test_pred = baselines.tfidf_svm_baseline(\n",
        "    datasets['train'], datasets['val'], datasets['test']\n",
        ")\n",
        "baselines.evaluate_model(datasets['val']['label_encoded'], svm_val_pred, 'TF-IDF + SVM')\n",
        "\n",
        "# Random Forest baseline\n",
        "print(\"\\n3. Random Forest baseline:\")\n",
        "rf_val_pred, rf_test_pred = baselines.random_forest_baseline(\n",
        "    datasets['train'], datasets['val'], datasets['test'], datasets['features']\n",
        ")\n",
        "baselines.evaluate_model(datasets['val']['label_encoded'], rf_val_pred, 'Random Forest')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bdf6b110",
      "metadata": {},
      "outputs": [],
      "source": [
        "class CodeBERTClassifier:\n",
        "    def __init__(self, model_name='microsoft/codebert-base'):\n",
        "        self.model_name = model_name\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
        "            model_name, num_labels=2\n",
        "        )\n",
        "    \n",
        "    def prepare_input(self, comment, code):\n",
        "        \"\"\"Prepare input for CodeBERT\"\"\"\n",
        "        # CodeBERT expects: [CLS] comment [SEP] code [SEP]\n",
        "        inputs = self.tokenizer(\n",
        "            comment, code,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=512,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        return inputs\n",
        "    \n",
        "    def create_dataset(self, df):\n",
        "        \"\"\"Create dataset for CodeBERT\"\"\"\n",
        "        class CodeBERTDataset(torch.utils.data.Dataset):\n",
        "            def __init__(self, df, tokenizer):\n",
        "                self.df = df\n",
        "                self.tokenizer = tokenizer\n",
        "            \n",
        "            def __len__(self):\n",
        "                return len(self.df)\n",
        "            \n",
        "            def __getitem__(self, idx):\n",
        "                row = self.df.iloc[idx]\n",
        "                inputs = self.tokenizer(\n",
        "                    str(row['comment_text']), str(row['code_snippet']),\n",
        "                    truncation=True,\n",
        "                    padding='max_length',\n",
        "                    max_length=512,\n",
        "                    return_tensors='pt'\n",
        "                )\n",
        "                return {\n",
        "                    'input_ids': inputs['input_ids'].flatten(),\n",
        "                    'attention_mask': inputs['attention_mask'].flatten(),\n",
        "                    'labels': torch.tensor(row['label_encoded'], dtype=torch.long)\n",
        "                }\n",
        "        \n",
        "        return CodeBERTDataset(df, self.tokenizer)\n",
        "    \n",
        "    def train_codebert(self, train_df, val_df, output_dir='./codebert_results'):\n",
        "        \"\"\"Train CodeBERT for comment usefulness\"\"\"\n",
        "        train_dataset = self.create_dataset(train_df)\n",
        "        val_dataset = self.create_dataset(val_df)\n",
        "        \n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=output_dir,\n",
        "            num_train_epochs=3,\n",
        "            per_device_train_batch_size=8,\n",
        "            per_device_eval_batch_size=16,\n",
        "            warmup_steps=500,\n",
        "            weight_decay=0.01,\n",
        "            logging_dir='./logs',\n",
        "            evaluation_strategy=\"epoch\",\n",
        "            save_strategy=\"epoch\",\n",
        "            load_best_model_at_end=True,\n",
        "            metric_for_best_model=\"eval_f1\",\n",
        "            logging_steps=50,\n",
        "        )\n",
        "        \n",
        "        def compute_metrics(eval_pred):\n",
        "            predictions, labels = eval_pred\n",
        "            predictions = np.argmax(predictions, axis=1)\n",
        "            precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')\n",
        "            accuracy = accuracy_score(labels, predictions)\n",
        "            return {\n",
        "                'accuracy': accuracy,\n",
        "                'f1': f1,\n",
        "                'precision': precision,\n",
        "                'recall': recall\n",
        "            }\n",
        "        \n",
        "        trainer = Trainer(\n",
        "            model=self.model,\n",
        "            args=training_args,\n",
        "            train_dataset=train_dataset,\n",
        "            eval_dataset=val_dataset,\n",
        "            compute_metrics=compute_metrics,\n",
        "        )\n",
        "        \n",
        "        print(\"Training CodeBERT model...\")\n",
        "        trainer.train()\n",
        "        return trainer\n",
        "    \n",
        "    def evaluate_model(self, trainer, test_df):\n",
        "        \"\"\"Evaluate trained CodeBERT model\"\"\"\n",
        "        test_dataset = self.create_dataset(test_df)\n",
        "        predictions = trainer.predict(test_dataset)\n",
        "        y_pred = np.argmax(predictions.predictions, axis=1)\n",
        "        y_true = predictions.label_ids\n",
        "        \n",
        "        accuracy = accuracy_score(y_true, y_pred)\n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
        "        \n",
        "        print(f\"\\nCodeBERT Model Results:\")\n",
        "        print(f\"Accuracy: {accuracy:.4f}\")\n",
        "        print(f\"Precision: {precision:.4f}\")\n",
        "        print(f\"Recall: {recall:.4f}\")\n",
        "        print(f\"F1-Score: {f1:.4f}\")\n",
        "        \n",
        "        return {\n",
        "            'accuracy': accuracy,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1': f1,\n",
        "            'predictions': y_pred,\n",
        "            'true_labels': y_true\n",
        "        }\n",
        "\n",
        "# Train CodeBERT model (uncomment to actually train)\n",
        "print(\"\\n4. CodeBERT Model:\")\n",
        "print(\"Note: CodeBERT training requires GPU and significant compute time.\")\n",
        "print(\"Uncomment the following lines to train:\")\n",
        "print(\"\")\n",
        "# codebert_classifier = CodeBERTClassifier()\n",
        "# codebert_trainer = codebert_classifier.train_codebert(datasets['train'], datasets['val'])\n",
        "# codebert_results = codebert_classifier.evaluate_model(codebert_trainer, datasets['test'])\n",
        "# baselines.results['CodeBERT'] = codebert_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c1b0eac",
      "metadata": {},
      "outputs": [],
      "source": [
        "class ModelEvaluator:\n",
        "    def __init__(self):\n",
        "        self.results = {}\n",
        "        self.predictions = {}\n",
        "    \n",
        "    def cross_validate(self, model, X, y, cv=5):\n",
        "        \"\"\"Perform k-fold cross validation\"\"\"\n",
        "        skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=42)\n",
        "        scores = {'accuracy': [], 'precision': [], 'recall': [], 'f1': []}\n",
        "        \n",
        "        for train_idx, test_idx in skf.split(X, y):\n",
        "            X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
        "            y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
        "            \n",
        "            model.fit(X_train, y_train)\n",
        "            y_pred = model.predict(X_test)\n",
        "            \n",
        "            scores['accuracy'].append(accuracy_score(y_test, y_pred))\n",
        "            precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
        "            scores['precision'].append(precision)\n",
        "            scores['recall'].append(recall)\n",
        "            scores['f1'].append(f1)\n",
        "        \n",
        "        return {metric: np.mean(values) for metric, values in scores.items()}\n",
        "    \n",
        "    def error_analysis(self, y_true, y_pred, df):\n",
        "        \"\"\"Perform detailed error analysis\"\"\"\n",
        "        errors_df = df.copy()\n",
        "        errors_df['predicted'] = y_pred\n",
        "        errors_df['correct'] = (y_true == y_pred)\n",
        "        \n",
        "        # False positives and false negatives\n",
        "        false_positives = errors_df[(errors_df['label_encoded'] == 0) & (errors_df['predicted'] == 1)]\n",
        "        false_negatives = errors_df[(errors_df['label_encoded'] == 1) & (errors_df['predicted'] == 0)]\n",
        "        \n",
        "        print(f\"False Positives: {len(false_positives)}\")\n",
        "        print(f\"False Negatives: {len(false_negatives)}\")\n",
        "        \n",
        "        # Analyze error patterns\n",
        "        print(\"\\nFalse Positive Examples (predicted useful, actually not useful):\")\n",
        "        for i, (idx, row) in enumerate(false_positives.head(3).iterrows()):\n",
        "            print(f\"{i+1}. Comment: {row['comment_text'][:100]}...\")\n",
        "            print(f\"   Code: {str(row['code_snippet'])[:100]}...\\n\")\n",
        "        \n",
        "        print(\"False Negative Examples (predicted not useful, actually useful):\")\n",
        "        for i, (idx, row) in enumerate(false_negatives.head(3).iterrows()):\n",
        "            print(f\"{i+1}. Comment: {row['comment_text'][:100]}...\")\n",
        "            print(f\"   Code: {str(row['code_snippet'])[:100]}...\\n\")\n",
        "        \n",
        "        return false_positives, false_negatives\n",
        "    \n",
        "    def create_confusion_matrix(self, y_true, y_pred, labels=['Not Useful', 'Useful']):\n",
        "        \"\"\"Create and visualize confusion matrix\"\"\"\n",
        "        from sklearn.metrics import confusion_matrix\n",
        "        \n",
        "        cm = confusion_matrix(y_true, y_pred)\n",
        "        \n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "                   xticklabels=labels, yticklabels=labels)\n",
        "        plt.title('Confusion Matrix - Random Forest')\n",
        "        plt.ylabel('True Label')\n",
        "        plt.xlabel('Predicted Label')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "        \n",
        "        return cm\n",
        "    \n",
        "    def create_results_table(self, results_dict):\n",
        "        \"\"\"Create comprehensive results table\"\"\"\n",
        "        results_df = pd.DataFrame(results_dict).T\n",
        "        results_df = results_df.round(4)\n",
        "        \n",
        "        print(\"\\n=== MODEL COMPARISON RESULTS ===\")\n",
        "        print(results_df.to_string())\n",
        "        \n",
        "        # Save results\n",
        "        results_df.to_csv('model_comparison_results.csv')\n",
        "        \n",
        "        # Create visualization\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        \n",
        "        metrics = ['accuracy', 'precision', 'recall', 'f1']\n",
        "        x = np.arange(len(results_df.index))\n",
        "        width = 0.2\n",
        "        \n",
        "        for i, metric in enumerate(metrics):\n",
        "            plt.bar(x + i*width, results_df[metric], width, label=metric.title())\n",
        "        \n",
        "        plt.xlabel('Models')\n",
        "        plt.ylabel('Score')\n",
        "        plt.title('Model Performance Comparison')\n",
        "        plt.xticks(x + width*1.5, results_df.index)\n",
        "        plt.legend()\n",
        "        plt.ylim(0, 1)\n",
        "        plt.grid(axis='y', alpha=0.3)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('model_comparison.png', dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "        \n",
        "        return results_df\n",
        "\n",
        "# Perform comprehensive evaluation\n",
        "evaluator = ModelEvaluator()\n",
        "\n",
        "# Error analysis for best performing model (Random Forest)\n",
        "print(\"\\n=== ERROR ANALYSIS ===\")\n",
        "y_true = datasets['test']['label_encoded'].values\n",
        "y_pred = rf_test_pred\n",
        "\n",
        "false_pos, false_neg = evaluator.error_analysis(y_true, y_pred, datasets['test'])\n",
        "\n",
        "# Confusion matrix\n",
        "cm = evaluator.create_confusion_matrix(y_true, y_pred)\n",
        "\n",
        "# Create final results table\n",
        "final_results = evaluator.create_results_table(baselines.results)\n",
        "\n",
        "print(f\"\\n=== SUMMARY ===\")\n",
        "print(f\"Best performing model: {max(baselines.results.keys(), key=lambda k: baselines.results[k]['f1'])}\")\n",
        "print(f\"Best F1-score: {max([r['f1'] for r in baselines.results.values()]):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "interpretability",
      "metadata": {},
      "outputs": [],
      "source": [
        "class ModelInterpreter:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    \n",
        "    def analyze_feature_importance(self, model, feature_names):\n",
        "        \"\"\"Analyze feature importance for tree-based models\"\"\"\n",
        "        if hasattr(model, 'feature_importances_'):\n",
        "            importance_df = pd.DataFrame({\n",
        "                'feature': feature_names,\n",
        "                'importance': model.feature_importances_\n",
        "            }).sort_values('importance', ascending=False)\n",
        "            \n",
        "            plt.figure(figsize=(10, 8))\n",
        "            sns.barplot(data=importance_df.head(15), x='importance', y='feature')\n",
        "            plt.title('Top 15 Feature Importances')\n",
        "            plt.xlabel('Importance')\n",
        "            plt.tight_layout()\n",
        "            plt.savefig('feature_importance.png', dpi=300, bbox_inches='tight')\n",
        "            plt.show()\n",
        "            \n",
        "            return importance_df\n",
        "    \n",
        "    def analyze_comment_patterns(self, df):\n",
        "        \"\"\"Analyze patterns in useful vs not useful comments\"\"\"\n",
        "        useful_comments = df[df['label'] == 'Useful']['comment_text']\n",
        "        not_useful_comments = df[df['label'] == 'Not Useful']['comment_text']\n",
        "        \n",
        "        print(\"=== COMMENT PATTERN ANALYSIS ===\")\n",
        "        \n",
        "        # Word frequency analysis\n",
        "        from collections import Counter\n",
        "        \n",
        "        def get_words(text_series):\n",
        "            all_text = ' '.join(text_series.astype(str).str.lower())\n",
        "            words = re.findall(r'\\b[a-z]+\\b', all_text)\n",
        "            return Counter(words)\n",
        "        \n",
        "        useful_words = get_words(useful_comments)\n",
        "        not_useful_words = get_words(not_useful_comments)\n",
        "        \n",
        "        print(\"Most common words in USEFUL comments:\")\n",
        "        for word, count in useful_words.most_common(10):\n",
        "            print(f\"{word}: {count}\")\n",
        "        \n",
        "        print(\"\\nMost common words in NOT USEFUL comments:\")\n",
        "        for word, count in not_useful_words.most_common(10):\n",
        "            print(f\"{word}: {count}\")\n",
        "        \n",
        "        # Comment length analysis\n",
        "        print(\"\\n=== LENGTH ANALYSIS ===\")\n",
        "        print(f\"Useful comments - Mean length: {useful_comments.str.len().mean():.1f} characters\")\n",
        "        print(f\"Not useful comments - Mean length: {not_useful_comments.str.len().mean():.1f} characters\")\n",
        "        \n",
        "        # Pattern analysis\n",
        "        print(\"\\n=== PATTERN ANALYSIS ===\")\n",
        "        useful_patterns = {\n",
        "            'Contains \"why\"': useful_comments.str.contains('why', case=False).sum(),\n",
        "            'Contains \"because\"': useful_comments.str.contains('because', case=False).sum(),\n",
        "            'Contains \"purpose\"': useful_comments.str.contains('purpose', case=False).sum(),\n",
        "            'Contains \"algorithm\"': useful_comments.str.contains('algorithm', case=False).sum(),\n",
        "        }\n",
        "        \n",
        "        not_useful_patterns = {\n",
        "            'Starts with verb': not_useful_comments.str.lower().str.startswith(('return', 'get', 'set', 'create')).sum(),\n",
        "            'Contains \"function\"': not_useful_comments.str.contains('function', case=False).sum(),\n",
        "            'Very short (<10 chars)': (not_useful_comments.str.len() < 10).sum(),\n",
        "        }\n",
        "        \n",
        "        print(\"Useful comment patterns:\")\n",
        "        for pattern, count in useful_patterns.items():\n",
        "            print(f\"{pattern}: {count} ({count/len(useful_comments)*100:.1f}%)\")\n",
        "        \n",
        "        print(\"\\nNot useful comment patterns:\")\n",
        "        for pattern, count in not_useful_patterns.items():\n",
        "            print(f\"{pattern}: {count} ({count/len(not_useful_comments)*100:.1f}%)\")\n",
        "\n",
        "# Perform interpretability analysis\n",
        "interpreter = ModelInterpreter()\n",
        "\n",
        "# Feature importance analysis (using Random Forest)\n",
        "if 'random_forest' in baselines.models:\n",
        "    print(\"\\n=== FEATURE IMPORTANCE ANALYSIS ===\")\n",
        "    importance_df = interpreter.analyze_feature_importance(\n",
        "        baselines.models['random_forest'], datasets['features']\n",
        "    )\n",
        "    \n",
        "    print(\"\\nTop 10 most important features:\")\n",
        "    print(importance_df.head(10).to_string(index=False))\n",
        "\n",
        "# Comment pattern analysis\n",
        "print(\"\\n=== LINGUISTIC PATTERN ANALYSIS ===\")\n",
        "interpreter.analyze_comment_patterns(datasets['combined'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "paper_generation",
      "metadata": {},
      "outputs": [],
      "source": [
        "class PaperResultsGenerator:\n",
        "    def __init__(self, results_dict, datasets):\n",
        "        self.results = results_dict\n",
        "        self.datasets = datasets\n",
        "    \n",
        "    def generate_dataset_statistics(self):\n",
        "        \"\"\"Generate dataset statistics for paper\"\"\"\n",
        "        stats = {\n",
        "            'total_samples': len(self.datasets['combined']),\n",
        "            'useful_samples': (self.datasets['combined']['label'] == 'Useful').sum(),\n",
        "            'not_useful_samples': (self.datasets['combined']['label'] == 'Not Useful').sum(),\n",
        "            'useful_ratio': (self.datasets['combined']['label'] == 'Useful').mean(),\n",
        "            'avg_comment_length': self.datasets['combined']['comment_text'].str.len().mean(),\n",
        "            'avg_code_length': self.datasets['combined']['code_snippet'].str.len().mean(),\n",
        "        }\n",
        "        \n",
        "        print(\"=== DATASET STATISTICS ===\")\n",
        "        print(f\"Total samples: {stats['total_samples']:,}\")\n",
        "        print(f\"Useful comments: {stats['useful_samples']:,} ({stats['useful_ratio']:.1%})\")\n",
        "        print(f\"Not useful comments: {stats['not_useful_samples']:,}\")\n",
        "        print(f\"Average comment length: {stats['avg_comment_length']:.1f} characters\")\n",
        "        print(f\"Average code length: {stats['avg_code_length']:.1f} characters\")\n",
        "        \n",
        "        return stats\n",
        "    \n",
        "    def generate_results_summary(self):\n",
        "        \"\"\"Generate results summary for paper\"\"\"\n",
        "        best_model = max(self.results.keys(), key=lambda k: self.results[k]['f1'])\n",
        "        best_f1 = self.results[best_model]['f1']\n",
        "        \n",
        "        print(\"\\n=== RESULTS SUMMARY ===\")\n",
        "        print(f\"Best performing model: {best_model}\")\n",
        "        print(f\"Best F1-score: {best_f1:.4f}\")\n",
        "        \n",
        "        print(\"\\nAll model results:\")\n",
        "        for model, metrics in self.results.items():\n",
        "            print(f\"{model}: F1={metrics['f1']:.4f}, Acc={metrics['accuracy']:.4f}\")\n",
        "        \n",
        "        return {\n",
        "            'best_model': best_model,\n",
        "            'best_f1': best_f1,\n",
        "            'all_results': self.results\n",
        "        }\n",
        "    \n",
        "    def generate_abstract(self, stats, results_summary):\n",
        "        \"\"\"Generate paper abstract\"\"\"\n",
        "        abstract = f\"\"\"\n",
        "ABSTRACT\n",
        "\n",
        "This paper presents a comprehensive approach to automatically assess the usefulness of code comments \n",
        "for software maintenance and comprehension. We introduce a dataset of {stats['total_samples']:,} \n",
        "code-comment pairs with binary usefulness labels, extracted from open-source C projects. Our evaluation \n",
        "compares multiple classification approaches, ranging from rule-based heuristics to machine learning models \n",
        "including TF-IDF with SVM and Random Forest with handcrafted features. The {results_summary['best_model']} \n",
        "model achieves the highest performance with an F1-score of {results_summary['best_f1']:.3f}. Our analysis \n",
        "reveals key linguistic and structural patterns that distinguish useful from non-useful comments, providing \n",
        "actionable insights for developers and automated code review tools. The dataset and implementations are \n",
        "made publicly available to support future research in this area.\n",
        "\n",
        "Keywords: Code Comments, Software Engineering, Information Retrieval, Machine Learning, Code Quality\n",
        "        \"\"\"\n",
        "        \n",
        "        print(abstract)\n",
        "        return abstract\n",
        "\n",
        "# Generate paper content\n",
        "paper_generator = PaperResultsGenerator(baselines.results, datasets)\n",
        "dataset_stats = paper_generator.generate_dataset_statistics()\n",
        "results_summary = paper_generator.generate_results_summary()\n",
        "abstract = paper_generator.generate_abstract(dataset_stats, results_summary)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"IRSE COMMENT USEFULNESS CLASSIFICATION - COMPLETE\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Dataset size: {len(datasets['combined']):,} samples\")\n",
        "print(f\"Best model: {results_summary['best_model']}\")\n",
        "print(f\"Best F1-score: {results_summary['best_f1']:.4f}\")\n",
        "print(\"All results saved to 'model_comparison_results.csv'\")\n",
        "print(\"Visualizations saved as PNG files\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
